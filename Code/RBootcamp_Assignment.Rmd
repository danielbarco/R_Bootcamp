---
title: "R Bootcamp"
date: "February 2020"
author:
- Daniel Barco & Silvan Burnand
output:
  html_document:
    df_print: paged
    theme: united
    toc: yes
    number_sections: true
    toc_float: true
  pdf_document: default
---

<br>

For Information regarding the use-case please find the attached use case.

## Load libraries

```{r message=FALSE, warning=FALSE}
# load libraries

library(getPass)
library(RCurl)
library(tidyverse)
library(magrittr)
library(janitor)
library(readxl)
library(lubridate)
library(DataExplorer)
library(ggfortify)  # autoplot
library(vistime)
library(plotly)
library(DT)
library(DT) # for dynamic tables
library(wordcloud) # to render wordclouds
library(tidytext) # for NLP
library(tm)  # for text mining
library(dygraphs)  # nice time series visualization
library(xts)  # more efficient time series format
library(reticulate) #to add python chunk
use_python("/Users/nijeri/dlv/bin/python3.7") #since the default version of python on mac os is python 2. it is necessary
# to specify python 3.7. unfortunately there is no way to solve this in a generic approach that works on all OS.
set.seed(69)
```

<br><br>


# Check & explore data

## Load data


```{r}
# load & preprocess data

d <- read_csv("../Data/dataset_lenzerheide.csv")

# lower case all variable names for easier programming
d %<>% rename_all(tolower)

d$datum <- dmy(d$datum)

d %<>%
  filter(datum < "2019-10-01")%>%
  arrange(desc(datum))

d %<>% filter(datum < "2016-05-01" | datum > "2017-04-30")
write_csv(d, "../Data/lenzerheide_zeiraeume_ohne_leere_labels.csv") 

introduce(d)
plot_intro(d)
plot_missing(d)
plot_correlation(d)
plot_histogram(d)

```




<br><br>

# Preprocessing
## Check timeseries data


```{r warning=TRUE}

d_schalter_daily_xts <- xts(x = d$schalter, order.by = d$datum)
names(d_schalter_daily_xts) <- "schalter"

d_schalter_daily_monthly_xts <- apply.monthly(d_schalter_daily_xts, sum)

# interactive graph
dygraph(d_schalter_daily_xts, main = "Daily aggregated line plot") %>%
  dyRangeSelector()

```

Jesus, we are missing some data from July 2016 until may 2017. No problem we can solve it: 

 ![](https://media.giphy.com/media/gMHFX7PEDZEHK/giphy.gif) 


Most sensible idea might be to exclude the missing time period from the 1st of may 2016 to the 1st of may 2017, as interpolation may cause a biased interpretation.

```{r}
ts <- read_csv("../Data/dataset_lenzerheide.csv")

# lower case all variable names for easier programming
ts %<>% rename_all(tolower)

ts$datum <- dmy(ts$datum)

ts %<>%
  filter(datum < "2019-10-01")%>%
  arrange(desc(datum))

ts %<>% filter(datum >= "2017-05-01" )

ts_schalter_daily_xts <- xts(x = ts$schalter, order.by = ts$datum)
names(ts_schalter_daily_xts) <- "schalter"

#ts_schalter_daily_monthly_xts <- apply.monthly(ts_schalter_daily_xts, sum)

# interactive graph
dygraph(ts_schalter_daily_xts, main = "Daily aggregated line plot") %>%
  dyRangeSelector()

```


<br><br>

## Check data types

There are some data as strings we will change them to numeric.

```{r}
str(ts)
```
<br><br>

### Replace NAs with 0s
```{r}
ts[is.na(ts)] <- 0
```
<br><br>

### Replace coloumn names whitspace with _
```{r}
names(ts) <- gsub(x = names(ts), pattern = " ", replacement = "_")  
names(ts)
```
<br><br>


### Add variable day_count

Number of day in year maybe an interesting feature, which can easily be added to the data. First we create three different sequences (one for each year). Afterwards the three sequences are concatenated to one vector which is added as variable "day_count".

```{r}

ls.17 <- sort(seq(from = 121, to = 365, by = 1), decreasing = TRUE)
ls.18 <- sort(seq(from = 1, to = 365, by = 1), decreasing = TRUE)
ls.19 <- sort(seq(from = 1, to = 273, by = 1), decreasing = TRUE)

ls.day_count <- c(ls.19,ls.18,ls.17)

ts$day_count <- ls.day_count


```



<br><br>

### Convert chr variables to factors and then to integers

We need those variables as integers, because they will be used for additional feature engineering later on.

```{r}
#luckily the first date 30.09.2019 corresponds to a monday so that the encoding is 1 = monday etc. 
ts$wochentag <- factor(ts$wochentag , levels = c(unique(ts$wochentag)), labels=c(seq(1,7)))
ts$ferien_desc_zh  = factor(ts$ferien_desc_zh , levels = c(unique(ts$ferien_desc_zh)), labels=c(seq(0,4)))
ts$ferien_desc_sg = factor(ts$ferien_desc_sg, levels = c(unique(ts$ferien_desc_sg)), labels=c(seq(0,4)))
ts$ferien_desc_gr = factor(ts$ferien_desc_gr, levels = c(unique(ts$ferien_desc_gr)), labels=c(seq(0,4)))
ts$feiertag_desc  = factor(ts$feiertag_desc , levels = c(unique(ts$feiertag_desc )), labels=c(seq(0,40)))
ts$isferiengr = factor(ts$isferiengr, levels = c(unique(ts$isferiengr)), labels = c(-1, 1))
ts$isferiensg = factor(ts$isferiensg, levels = c(unique(ts$isferiensg)), labels= c(-1, 1))
ts$isferienzh = factor(ts$isferienzh, levels = c(unique(ts$isferienzh)), labels= c(-1, 1))
ts$isfeiertag_gr = factor(ts$isfeiertag_gr, levels = c(unique(ts$isfeiertag_gr)), labels= c(-1, 1))
ts$isfeiertag_sg = factor(ts$isfeiertag_sg, levels = c(unique(ts$isfeiertag_sg)), labels = c(-1, 1))
ts$isfeiertag_zh = factor(ts$isfeiertag_zh, levels = c(unique(ts$isfeiertag_zh)), labels= c(-1, 1))
#ts$day_count = factor(ts$day_count, levels = c(unique(ts$day_count)), labels=c(seq(1,365)))
levels(ts$isfeiertag_zh)
```

<br><br>





## Divide and conquer

 ![](https://media.giphy.com/media/3o6fIVCYOLGPO1cfYc/giphy.gif) 

We will use Schalter as target variable. This is the amount of tourist that physically visit the tourist information. The other target variables will be excluded: tel, mail, total_anfragen.

```{r}
train <- filter(ts, datum < "2018-12-01")
validate <- filter(ts, (datum  > "2018-12-01" & datum < "2019-05-01"))
test <- filter(ts, datum >= "2019-05-01" )
dd_train <- train[,c(-1, -36, -35, -34)]
dd_validate <- validate[,c(-1,-36, -35, -34)]
dd_test <- test[,c(-1, -36, -35, -34)]
write_csv(dd_train, "../Data/train.csv") 
write_csv(dd_validate, "../Data/validate.csv") 
write_csv(dd_test, "../Data/test.csv") 

```

<br><br>

# Linear Model

```{r}

lm.fit <- lm(schalter ~. ,
               data=dd_train)
par(mfrow=c(2,2))
plot(lm.fit, col="blue")
summary(lm.fit)
```
### Let us check the coefficients
```{r}
lm.fit$coefficients
```
NAs are produced due to missing elements in the validation dataset, as this dataset is only within a specific timeframe, where not all events occur.


# Random  Forest
Create Random Forest Tree mtry to 30 (35 variables -date, etc.)
```{r}
library(randomForest)
rf.try = randomForest(schalter~. -day_count, data=dd_train, importance=TRUE)
rf.try
```


Check which Random Forest ist most promising
```{r}
n <- 30
oob.err=double(n)
test.err=double(n)

#mtry is no of Variables randomly chosen at each split
for(mtry in 1:n) 
{
  rf=randomForest(schalter ~ . , data= dd_train,mtry=mtry,ntree=500) 
  oob.err[mtry] = rf$mse[500] #Error of all Trees fitted
  
  pred<-predict(rf,dd_validate) #Predictions on Test Set for each Tree
  test.err[mtry]= with(dd_validate, mean( (schalter - pred)^2)) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
}
```


Plot out the results
```{r}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Mean Squared Error",xlab="Number of Predictors Considered at each Split", main = "Random Forest Models with mtry 1 to 15")
legend("topright",legend=c("Out of Bag Error","Test Error"),pch=19, col=c("red","blue"))

```

### Looks like 25 predictors should be sufficient.

```{r}
rf.fit =randomForest(schalter~.,data=dd_train,mtry=25,importance=TRUE,ntree=500)
rf.fit
#importance(rf.dd)
varImpPlot(rf.fit, n.var = 8, sort = TRUE, main = "Random Forest 8 most important variables" )
```

```{r}
rf.pred = predict(rf.fit,newdata=dd_test)
test.err= mean( (dd_test$schalter - rf.pred)^2)
print(test.err)
```

Plot Residuals Stack & Mean Residuals (blue) & Regression line (red)
```{r}
val_plot <- dd_test
val_plot$rf.res <- dd_test$schalter - rf.pred
n <- nrow(val_plot)
val_plot$count <- (1:n)
plot(val_plot$rf.res~val_plot$count, xlab = "Prediction days counted from 01.05.2019", ylab = "Residuals", main = "Residuals Stack & Mean Residuals (blue) & Regression line (red)")
abline(lm(val_plot$rf.res~val_plot$count), col = "red")
abline(h= mean(val_plot$rf.res),col="blue")
```


Add the mean difference
```{r}
# calculate the percentage difference of predicted mean to test mean
rf.mean <- mean(dd_test$schalter-rf.pred)/mean(rf.pred)
rf.mean

rf.mse <- with(dd_test, mean( (schalter - rf.pred)^2))
rf.mse

```

Add the median difference
```{r}
# calculate the percentage difference of predicted mean to test mean
rf.median <- median(dd_test$schalter-rf.pred)/median(rf.pred)
rf.median

# calculate residuals
rf.median.res <- dd_test$schalter - ((1+rf.median)*rf.pred)
rf.res <- dd_test$schalter - rf.pred
```

Visualise Error
```{r}
library(RColorBrewer)
library(ggplot2)
val_pred <- dd_test
val_pred$prediction <- rf.res
n <- nrow(dd_test)
val_pred$count <- (1:n)

myColors <- brewer.pal(7, "Set1")
names(myColors) <- levels(d$wochentag)
colScale <- scale_colour_manual(name = "Weekday N", values = myColors)

#and then add the color scale onto the plot as needed:


#One plot with all the data
p <- ggplot(val_pred,
            aes(count, prediction, colour = as.factor(wochentag))) + 
    geom_point(alpha = 0.7) +
  ggtitle("Residuals coloured by weekdays") +
  xlab("Prediction days counted from 01.05.2019") + ylab("Residuals") 
  # Change the legend

p1 <- p   + colScale
p1
```

In this plot we can see that the residuals are not greater for any one single weekday, indicating that we do not have a bias, where a certain weekday is predicted better than another. The weekdays are encoded as follows 1 = Monday, 2 = Tuesday etc..

# Comparing Linear Model to Python

```{python}
import pandas as pd
import numpy as np
import sklearn as sk
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import random
random.seed(69)

# importing dataframes from R
train = r['dd_train']
validate = r['dd_validate']

# # Encoding categorical columns I: LabelEncoder 
# def label_encoder(df):
#   # Fill missing values with 0
#   df.LotFrontage = df.LotFrontage.fillna(value=0)
#   # Create a boolean mask for categorical columns 
#   categorical_mask = (df.dtypes == object)  # true â†’ is categorical
#   # Get list of categorical column names
#   categorical_columns = df.columns[categorical_mask].tolist()
#   # Create LabelEncoder object: le 
#   le = LabelEncoder()
#   # Apply LabelEncoder to categorical columns
#   df[categorical_columns] = df[categorical_columns].apply(lambda x: le.fit_transform(x))
#   return df
# 
# 
# # One Hot Encoding
# def one_hot_encoder(df):
#   # Create OneHotEncoder: ohe
#   ohe = OneHotEncoder(categorical_features=categorical_mask, sparse=False)
#   # Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded df_encoded = ohe.fit_transform(df)
#   return df
#   
# train = label_encoder(train)
# validate = label_encoder(validate)
# train = one_hot_encoder(train)
# validate = one_hot_encoder(validate)

X_train = train.drop(columns=['schalter'])
y_train = train['schalter']
X_validate = validate.drop(columns=['schalter'])
y_validate = validate['schalter']

reg = LinearRegression().fit(X_train, y_train)
reg.score(X_train, y_train)

# Make predictions using the testing set
y_pred = reg.predict(X_validate)

coefficients = pd.concat([pd.DataFrame(train.columns),pd.DataFrame(np.transpose(reg.coef_))], axis = 1)

# The coefficients
print('Coefficients: \n', coefficients)

# The mean squared error
print('Mean squared error: %.2f'
      % mean_squared_error(y_validate, y_pred))
      
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
      % r2_score(y_validate, y_pred))

```

Some notes concerning the output: Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R). source(https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)

The python output for this model is not as easily interpretable as in the R script. The R script follows a convenient layout whereas in pyhton this information must be extracted from the model. For statistical modelling, R comes with some convenient advantages.

Unfortunately not all pyhton functionalities are possible in R, which makes preprocessing and especially preprocessing impossible i.e. label encoding and one hot encoding. Therefore the lm model in the r script and the one in python are not fully comparable.

 ![Python rules](https://media.giphy.com/media/3o72F7JTbNletrGzvO/giphy.gif) 
  ![](https://media.giphy.com/media/10tuFEeuACAnuw/giphy.gif) 

